Pseudocode for Multivariate Nonlinear Newton's Method Optimization
____________________________________________________________________________

Goal: Optimize a give cost function by adjusting the input on a
  statespace with the intended parameters. In other words, find the
  point where the gradient of the system of cost functions is equal
  to zero.

    C(q) -> C(u)      (cost system is indirectly controlled by u)
    J(q) -> J(u) = 0  (gradient system of equations)

  To find this point, the system and its gradient will be found by
  iterating the cost function and solving for its gradient using a
  system similar to ode45() in MATLAB (for initial testing).

  Needed Parameters:
    System of equations for cost.
    Method of calculating gradient for cost function.

  When J(u) is calculated, the next step of the Newton's method is...
    U = u - C(u) / J(u)
  where U is the new input value to be iterated on.
____________________________________________________________________________

Target: Minimum point for system C(u)

01| create initial guess (set to uc)
02| calculate cost of initial guess (Cc)
03| calculate Jacobian around initial guess (Jc)
04| calculate the next guess (un = uc - Inv(Jc)*Cc)
05| while (difference between un and uc is large)
06|   repeat steps 1-4
  | end while
07| return the acceptable inputs (u = un)
____________________________________________________________________________

Calculating the Jacobian Matrix
Numerical Differentiation - Three-Point Backward Difference
Target: du around current guess (Jc)

Note: Method is problematic because it requires previous state data. At
      the start of the simulation, the previous states are not available.

Function for derivative around current state is dependent on the cost of
state variables at the current position.

01|
02|
03|
04|
____________________________________________________________________________

Calculating the Jacobian Matrix
Automatic Differentiation
Target: du around current guess (Jc)
