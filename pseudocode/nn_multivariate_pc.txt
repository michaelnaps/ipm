Pseudocode for Multivariate Nonlinear Newton's Optimization Method
____________________________________________________________________________

Goal: Optimize a given cost function by adjusting the input on a
  state space with the intended parameters. In other words, find the
  point where the gradient of the system cost functions is equal
  to zero.

    C(q) -> C(u,q)      (cost system is directly controlled by u)
    g(q) -> g(u,q) = 0  (gradient system)

  To find this point, the system and its gradient/hessian will
  be found via the Central Finite Difference Method. This method calculates
  the cost for small changes in the individual inputs. The hessian
  is similarly calculating by finding the gradient for small changes
  in the individual inputs.

  Needed Parameters:
    System of equations for cost.
    Method of calculating gradient for cost function.

  When J(u) is calculated, the next step of the Newton's method is...
    U = u - g(u) / H(u)
  where U is the new input value for the next iteration.
____________________________________________________________________________

Optimization Method: Newton's Optimization Method
Target: Minimum cost input for system C(u)
Prioritized Break Condition: First Order Optimality (FOO)

01| create initial guess (set to uc)
02| calculate cost of initial guess (Cc)
  | enter loop till break condition is met (see 3, 8, 10, 12)
03|   if (Cc is close to zero)
04|     return input and exit loop
  |   end if
05|   calculate gradient around guess (g)
06|   calculate hessian around guess (H)
07|   calculate the next guess (un = uc - inv(H)*g)
08|   if (gradient is close to zero) -> FOO
09|     return input and exit loop
  |   end if
10|   if (change in input is close to zero)
11|     return input and exit loop
  |   end if
12|   if (iteration count is too large, i.e. diverged)
13|     return input and exit loop
  |   end if
  | end loop
____________________________________________________________________________

Method: 2-point Central Finite Difference Method
Target: Gradient/Hessian Matrix
Note: Hessian matrix is calculated using the same method, but by calculating
      the change in the gradient.

01| initialize current state and guess input
02| calculate cost of current state with guess input
03| for (number of inputs)
04|   create small change in the original input (h = step size)
05|   add/subtract the step size from the original input
06|   calculate the cost of the new inputs
07|   calculate the gradient using 2-point CDM (g = (Cp - Cn)/2*h)
  | end for
08| return completed gradient matrix approximation
